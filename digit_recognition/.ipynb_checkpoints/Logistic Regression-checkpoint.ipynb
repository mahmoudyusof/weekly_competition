{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for Classification problems\n",
    "First of all let's start with some definitions and terminology.  \n",
    "- What is machine learning?\n",
    "- What are the types of machine learning problems?\n",
    "- When to use machine learning?\n",
    "- Logistic Regression for binary classification.\n",
    "- Logistic Regression for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine learning\n",
    "Machine learning is the use and development of computer systems that are able to learn and adapt without following explicit instructions, by using algorithms and statistical models to analyse and draw inferences from patterns in data.  \n",
    "This essentially means that the inputs of a machine learning system are data and results, and the outputs are rules. unlike classical programming where the inputs are data and rules, and the outputs are results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of machine learning problems\n",
    "- Supervised learning\n",
    "    - Classification\n",
    "    - Regression\n",
    "    - ...\n",
    "- Unsupervised learning\n",
    "    - Clustering\n",
    "    - ...\n",
    "    \n",
    "### Supervised learning\n",
    "Supervised learning is a type of machine learning algorithms that is used when you have both the data and the target output and your goal is to learn the correlation between the data and the target output.  \n",
    "One example is Classification which is going to be this notebook's example. You will be given 28 * 28 pixel data for a hand written digit from 0 to 9 and the goal is to know only from the pixel data what digit the data represents. So you should classify the data, hence the name.\n",
    "\n",
    "### Unsupervised learning\n",
    "Unsupervised learning algorithms are used when you don't have the target output for the training data. \n",
    "One example for clustering is when you have data of your ecommerce website users and you want to figure out how may sigments of users exists and what are there interests. In this case you're going to use the user features to cluster similar users together, however, notice that you don't know the number of clusters nor the names of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use machine learning\n",
    "Machine learning should be use only if necessary, and you know that from the business problem itself. A common mistake is that some engineers try to enforce a machine learning solution to a problem that doesn't need it. Make sure you understand the business problem carefully and see if machine learning could solve it or not.  \n",
    "\n",
    "This notebook, as you might guess, doesn't actually solve much of a business problem, however, it is a good practice for machine learning beginners, it is called \"Hello world of machine learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before we dive into LogisticRegression, we need to take a look at the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv(\"./train.csv\")\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a label column in the training dataframe which is the target output and you have pixel values from pixel0 all the way to pixel783 which is basically a flattened 28 * 28 image. each pixel has a value between 0 and 255, 0 being black and 255 being white.  \n",
    "Let's take a look at some of these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_example(df, train=True):\n",
    "    index = np.random.randint(len(df))\n",
    "    example = df.iloc[index]\n",
    "    if train:\n",
    "        label = example[0]\n",
    "        img = example[1:].values.reshape((28, 28))\n",
    "    else:\n",
    "        img = example.values.reshape((28, 28))\n",
    "        label = \"Unknown\"\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_training_example(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why the data is provided as a flattened array instead of an image is that almost all machine learning algorithms require a flat array (if we exclude deep learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8816\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to get images of only zeros and ones to do binar classification\n",
    "binary_df = train_df.loc[train_df['label'] < 2]\n",
    "show_training_example(binary_df)\n",
    "print(len(binary_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "> **Note** some basic linear algebra ahead\n",
    "\n",
    "Let's assume the input features make a vector $X$ where the dimension of $X$ is $(n, 1)$ where $n$ is the number of features.  \n",
    "Right now we have only one output which takes values 0 or 1 let's call it the hypothesis $h$ of shape $(c, 1)$ where $c$ the number of classes (one in this case). we need some tranformation matrix that takes us from $X$ to $O$, we will call this matrix the weights matrix $W$ where $W$ is of shape $(n, c)$.  \n",
    "Our hypothesis $h = X \\odot W$ however there are two problems with this hypothesis:\n",
    "- It always passes through the origin\n",
    "- It outputs values from $-\\infty$ to $\\infty$ and we need numbers between 0 and 1\n",
    "\n",
    "to solve the first problem we add the bias vector $b$ of shape $(1, 1)$ per example  \n",
    "To solve the second we use the logistic sigmoid function which takes the real number line and squeezes it between 0 and 1.  \n",
    "$$ h = \\sigma(X \\odot W + b) $$\n",
    "Where\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1 + np.exp(-z))\n",
    "\n",
    "def hypothesis(X, W, b):\n",
    "    return sigmoid(np.dot(X, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the hypothesis represents is the probability of the data example being of class 1, however, if we use it as is it will give random results simply because $W$ and $b$ are randomly initialized.  \n",
    "We need a way to iteratively improve the values of $W$ and $b$ so that they would describe the data better.  \n",
    "To do so we need to know how bad the current values are, and this is measured using the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write the loss function for one example and then we will write the general cost function and then explain both.\n",
    "- Loss function\n",
    "$$ l = - y\\log(h(x)) - (1 - y)\\log(1 - h(x)) $$\n",
    "where $x$ is the feature vector for one example and $y$ is the target label.  \n",
    "\n",
    "- Cost function over all examples\n",
    "$$ L = \\frac{1}{m}\\sum_{i=1}^m - y_i\\log(h(x_i)) - (1 - y_i)\\log(1 - h(x_i)) $$\n",
    "So the cost function is simply the mean of the loss over all training examples.  \n",
    "\n",
    "Now let's see why this works.  \n",
    "When $y_i == 1$ and $h(x_i) \\simeq 0$ this means we got it wrong, this will cause the second term of the equation to equal 0 however the first term is goint to be $\\log(~0)$ which is going to be a very big negative number.  \n",
    "When $y_i == 1$ and $h(x_i) \\simeq 1$ this means we got it right, this will cause the second term of the equation to equal 0 however the first term is goint to be $\\log(~1)$ which is going to be a very small negative number.  \n",
    "\n",
    "Now this proves that the Loss function we wrote earlier is a good indicator for how bad the model is which means we need to run some sort of optimization algorithm on this function to minimize it with respect to $W$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(h, y):\n",
    "    return (-y * np.log(h + 0.001) - (1 - y) * np.log(1 - h + 0.001)).mean()\n",
    "\n",
    "## the small number in the log is to prevent log(0) which would overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "![Gradient Descent](https://cdn-images-1.medium.com/max/600/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg)  \n",
    "\n",
    "From vector analysis we know that the gradient of a function is the direction at which the function increases the most denoted by the symbol $\\nabla$.  \n",
    "If we can calculate the gradient of the cost function $\\nabla L$ at any point and move in the opposite direction updating the weights and bias this is going to slowly decrease the loss until we reach a local minimum.  \n",
    "Update equations\n",
    "$$ W := W - \\alpha * \\nabla_{W} L $$\n",
    "$$ b := b - \\alpha * \\nabla_{b} L $$\n",
    "$ \\nabla_{W} L = X^T \\odot (h - y) $  \n",
    "$ \\nabla_{b} L = (h - y) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7052, 784)\n",
      "(7052,)\n",
      "(1764, 784)\n",
      "(1764,)\n"
     ]
    }
   ],
   "source": [
    "# now let's start training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = binary_df.drop(columns=['label']).values / 255.\n",
    "y = binary_df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2398060700107147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-fc1c4a264ad5>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03442734184224975\n",
      "0.028391118405072704\n",
      "0.022512994661752727\n",
      "0.018594245496099757\n",
      "0.01873367477600647\n",
      "0.017702716572840668\n",
      "0.016643539941405057\n",
      "0.01664282332837309\n",
      "0.016642823297358025\n",
      "0.01595095513383714\n",
      "0.013714153328323578\n",
      "0.013714157975348156\n",
      "0.012726439127533578\n",
      "0.011522951557567874\n",
      "0.009050111067904213\n",
      "0.00785760535237063\n",
      "0.007856825384040644\n",
      "0.007856547728370227\n",
      "0.007856424267413302\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(X_train.shape[1])\n",
    "b = np.random.rand(1)\n",
    "alpha = 0.1\n",
    "epochs = 200\n",
    "costs = []\n",
    "for i in range(200):\n",
    "    h = hypothesis(X_train, W, b)\n",
    "    costs.append(cost(h, y_train))\n",
    "    W -= alpha * np.dot(X_train.T, (h - y_train))\n",
    "    b -= alpha * (h - y_train).mean()\n",
    "    if i % 10 == 0:\n",
    "        print(costs[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Now let's see how the loss is decreasing\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(198)), costs[2:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-fc1c4a264ad5>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "h_test = hypothesis(X_test, W, b)\n",
    "for i in range(h_test.size):\n",
    "    h_test[i] = int(h_test[i] >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9977324263038548"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, h_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we can now get a model that can classify images of hand written zeros and ones with an accuracy of 99.9% which is not bad at all.  \n",
    "Now we will use [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty=\"none\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs are very similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi class classification\n",
    "If you read the docs for the LogisticRegression classifier, it will tell you that for multiclass classification (which means you have more than two classes) it can use the one-vs-Rest method (OvR) if you set the `multi_class` parameter to 'ovr', however, by default it uses crossentropy loss for multiclass problems which is more complex than the simple log loss function.  \n",
    "OvR means that for the mnist problem it would train one classifier that can say whether a digit is 0 or not, and another for ones and another for twos and so on, so it would train 10 models, one for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[\"label\"]).values / 255.\n",
    "y = train_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it\n",
    "\n",
    "## memory consuming code, don't run it\n",
    "## or do, who cares\n",
    "\n",
    "# model = LogisticRegression(penalty=\"none\", multi_class=\"ovr\")\n",
    "# model.fit(scale(X_train), y_train)\n",
    "\n",
    "# accuracy_score(y_test, model.predict(scale(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahmoud/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9164285714285715"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's try crossentropy\n",
    "model = LogisticRegression(penalty=\"none\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just out of curiousity, what is the accuracy score on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9417857142857143"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be much higher, this happened because the mode memorized the training data and failed to generalize to new data, this phenominon is called overfitting and can be solved using regularization\n",
    "### Regularization\n",
    "regularization is so simple, all we need to do is add a term to the loss function that penalizes the model for having high weights.\n",
    "$$ L = \\frac{1}{m}\\sum_{i=1}^m - y_i\\log(h(x_i)) - (1 - y_i)\\log(1 - h(x_i)) + \\lambda \\|{W}\\|^2 $$  \n",
    "In scikit learn you just need to set the `penalty` parameter to 'l2' which is the default value.  \n",
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9168452380952381, Testing Accuracy: 0.9096428571428572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahmoud/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l2', C=0.01)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_score = accuracy_score(y_train, model.predict(X_train))\n",
    "test_score = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(\"Training Accuracy: {}, Testing Accuracy: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the documentation you will see that the smaller the `C` parameter the stronger the regularization, which means it is the inverse of the $\\lambda$ in the equation. try different values of `C` and see which results in the least overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End\n",
    "I hope this was useful, thank you for coming this far and happy learning :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
